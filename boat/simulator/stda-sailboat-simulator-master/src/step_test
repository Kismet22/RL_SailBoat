# step_test
"""""""""
    def step(self, x):

        #########
        # 计算当前距离
        x_old = x
        distance_old = calculate_distance(x_old, self.end_target)
        print('当前位置距离', distance_old)
        #########

        # self.ref_heading = 0
        integrator = self.integrator
        predefined_sail_angle = None
        # 输入上一个状态x
        print('position', (x[0], x[1]))
        # ref_heading 代表的是期望前往的航向
        # print('heading', self.ref_heading[self.i])
        print('heading', self.ref_heading)

        #########
        # PID控制器控制
        speed = sqrt(x[VEL_X] ** 2 + x[VEL_Y] ** 2)
        drift = arctan2(x[VEL_Y], x[VEL_X])
        # x[YAW]中保存当前的航向
        # environment[RUDDER_ANGLE] = self.controller.controll(self.ref_heading[self.i], x[YAW], x[YAW_RATE], speed,
        #                                                     x[ROLL], drift_angle=drift)
        environment[RUDDER_ANGLE] = self.controller.controll(self.ref_heading, x[YAW], x[YAW_RATE], speed,
                                                             x[ROLL], drift_angle=drift)
        #########

        #########
        # RL随机角度输入
        # random_rudder_angle = random.uniform(-pi*45/180, pi*45/180)  # 在min_angle和max_angle之间生成随机角度
        # environment[RUDDER_ANGLE] = random_rudder_angle
        #########

        # 船体舵角(action1)
        rudder = environment[RUDDER_ANGLE]
        print('rudder_angle', rudder)

        if not predefined_sail_angle is None:
            sail = predefined_sail_angle
            self.sail_angle = sail
            environment[SAIL_ANGLE] = sail  # 目标角度
        # 当前的时间步数是 sail_sampletime（风帆控制采样时间）的整数倍时，控制器会计算新的风帆角度
        else:
            if not self.i % int(self.sail_sampletime / self.sampletime):
                # 添加当前风的影响
                apparent_wind = calculate_apparent_wind(x[YAW], x[VEL_X], x[VEL_Y], self.wind)
                environment[SAIL_ANGLE] = sail_angle(apparent_wind.angle, apparent_wind.speed, SAIL_STRETCHING)

                sail = environment[SAIL_ANGLE]
                self.sail_angle = sail

            else:
                sail = self.sail_angle

        # 船体帆角(action2)
        print('sail_angle:', sail)
        print('next_state:', integrator.y)
        # 通过积分器更新下一个状态

        # state状态更新
        integrator.integrate(integrator.t + self.sampletime)
        # print('integrator.y after integration:', integrator.y)
        # t = self.integrator.t
        x_new = self.integrator.y
        # self.t = t
        # 保存更新的integrator
        self.integrator = integrator
        self.i = self.i + 1
        # 更新期望角度
        self.ref_heading = calculate_angle(x_new, self.end_target)

        #########
        # 计算更新后的距离
        distance_new = calculate_distance(x_new, self.end_target)
        #########

        #########
        # 计算两次运动的距离
        # distance = calculate_distance(x_old, x_new)
        # print('两次运动的距离', distance)
        #########

        #########
        # 计算奖励函数
        time_punish = -self.sampletime  # 时间惩罚项
        distance_reward = 10 * (distance_old - distance_new) / self.sampletime  # 距离奖励项
        dynamics_reward = 0  # 其他力学奖励项
        reward = time_punish + distance_reward + dynamics_reward
        # 到达期望区域坐标内
        if self.break_flag:
            reward = reward + self.success_reward
        #########
        return x_new
"""

"""""""""
    def get_og_state(self):
        ########
        # 加入随机设置的起点
        # target = random_start()
        target = [2, 2]
        # print('目标位置', target)
        self.end_target = target
        self.sail_angle = None
        ########

        ########
        # 状态记录
        self.i = 0
        x0 = zeros(self.n_states)
        x0[VEL_X] = 0.
        if actor_dynamics:
            x0[SAIL_STATE] = 48 * pi / 180
        integrator, controller = init_integrator(x0, self.sampletime)
        self.integrator = integrator  # 运动更新器
        self.controller = controller  # PID状态控制器
        self.ref_heading = calculate_angle(x0, self.end_target)
        self.state = x0
        print('self.state', self.state)
        x_state = x0
        ########

        ########
        # 实际使用到的state
        x_state[0] = self.end_target[0] - x_state[0]  # 将绝对位置坐标改为相对位置坐标
        x_state[1] = self.end_target[1] - x_state[1]
        ########
        return x_state

    def reset(self):
        observation = self.get_og_state()
        print('self.state', self.state)
        info = None
        return observation, info
"""


当你设置了total_timesteps、n_steps和batch_size后，训练过程通常会遵循以下步骤：

数据收集： 在每个训练迭代中，使用当前策略与环境交互n_steps个时间步，收集这段经验。这个经验包括状态、动作、奖励等信息。

计算优势估计： 使用这段经验，计算每个时间步的优势估计（Advantage Estimation）。Advantage估计代表了在某个状态下采取某个动作相对于平均预期的好坏程度。这是强化学习中很重要的概念，它帮助算法判断哪些动作是好的，哪些是坏的。

计算损失： 使用这些经验数据和优势估计，计算策略网络（通常是神经网络）的损失函数。损失函数通常包括两个部分：策略损失和值函数损失。策略损失用来调整策略，使得采取好的动作的概率增加。值函数损失用来调整值函数，使得预测的奖励更接近实际的奖励。

梯度计算和更新： 计算损失的梯度，然后使用梯度下降法或其他优化算法来更新策略网络的参数，使得损失减小。

重复迭代： 重复上述步骤，直到达到指定的total_timesteps。

在这个过程中，batch_size决定了每次梯度更新时使用的样本数量。较大的batch_size通常能够提供更稳定的梯度估计，但也会增加计算开销。较小的batch_size可能会引入一些随机性，但会更快地进行训练。权衡通常取决于你的硬件性能和问题的特性。